{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem: Person Re-Identification"
      ],
      "metadata": {
        "id": "vpMk0cREfR_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to identify which detection IDs belong to the same person across multiple camera feeds based on the provided metadata. The challenge is to group detections by person, despite different camera angles and tracking IDs that may vary across cameras.\n",
        "\n"
      ],
      "metadata": {
        "id": "hx-sgBzSfekN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load and Parse the Input JSON Files**\n",
        "\n",
        "We begin by loading the detection metadata from the `final_detections.json` file. This file contains details about each detection, including the detection ID, camera ID, frame ID, bounding box, and a 768-dimensional feature vector representing the detected person."
      ],
      "metadata": {
        "id": "oUL_eA65gk_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "Odi2INQwk88-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the detection data from the final_detections.json file\n",
        "with open('final_detections.json', 'r') as f:\n",
        "    detections = json.load(f)\n",
        "\n",
        "# Extracting detection IDs, feature vectors, and other metadata\n",
        "detection_ids = [det['detection_id'] for det in detections]\n",
        "features = np.array([det['feature'] for det in detections])"
      ],
      "metadata": {
        "id": "mw5HNbckhV7X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Compute Cosine Similarity**\n",
        "\n",
        "In this step, we calculate the cosine similarity between the feature vectors of the detections. Cosine similarity helps measure how similar two detections are, which is key to determining if they belong to the same person."
      ],
      "metadata": {
        "id": "Po5_1eIChrCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cosine similarity between feature vectors\n",
        "similarity_matrix = cosine_similarity(features)\n",
        "\n",
        "# Printing the shape of the similarity matrix\n",
        "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")"
      ],
      "metadata": {
        "id": "VVxgDTmihjNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "\n",
        "*   The cosine similarity matrix is a square matrix where each entry (i, j) represents the similarity between detection i and detection j. A value close to 1 means the detections are highly similar and likely belong to the same person."
      ],
      "metadata": {
        "id": "yI8IIIfch5l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Clustering Based on Cosine Similarity**\n",
        "\n",
        "We use Agglomerative Clustering, which groups similar detections into clusters without needing to predefine the number of clusters. This step forms clusters of detections based on the similarity of their feature vectors."
      ],
      "metadata": {
        "id": "1C6r0ZMviKfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting cosine similarity into distance (1 - cosine similarity) (Agglomerative Clustering works on distance)\n",
        "distance_matrix = 1 - similarity_matrix\n",
        "\n",
        "# Clustering model\n",
        "clustering_model = AgglomerativeClustering(\n",
        "    affinity='precomputed',\n",
        "    linkage='average',\n",
        "    distance_threshold=0.5,\n",
        "    n_clusters=None\n",
        ")\n",
        "\n",
        "# Fitting the clustering model using the distance matrix\n",
        "clusters = clustering_model.fit_predict(distance_matrix)\n",
        "\n",
        "# Priinting the number of clusters formed\n",
        "print(f\"Number of clusters formed: {len(np.unique(clusters))}\")"
      ],
      "metadata": {
        "id": "mRc8-YaIiNmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "\n",
        "* **Agglomerative Clustering:** This is a hierarchical clustering technique that works with a distance matrix. We are converting cosine similarity to distance by subtracting similarity from 1.\n",
        "* **Linkage:** We use \"average\" linkage, which clusters based on the average distance between all pairs of observations from the two sets.\n",
        "* **Distance Threshold:** This parameter determines when two clusters should be merged. You can experiment with this value to fine-tune the clustering results."
      ],
      "metadata": {
        "id": "m9lh-v0aiQI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Grouping Detections by Clusters**\n",
        "\n",
        "Once the clustering is done, we group the detection IDs by their respective cluster labels. Each cluster corresponds to a group of detections that are predicted to belong to the same person. We then save the predictions in the required `prediction.json` format."
      ],
      "metadata": {
        "id": "8DVJc4IkiqXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping detection IDs by their cluster labels\n",
        "grouped_detections = defaultdict(list)\n",
        "for idx, cluster_id in enumerate(clusters):\n",
        "    grouped_detections[cluster_id].append(detection_ids[idx])\n",
        "\n",
        "# Converting the groups to a list of lists\n",
        "predictions = list(grouped_detections.values())\n",
        "\n",
        "# Saving the predictions to a JSON file\n",
        "with open('prediction.json', 'w') as f:\n",
        "    json.dump(predictions, f)\n",
        "\n",
        "# Printing the predictions\n",
        "print(\"Predictions saved to prediction.json:\")\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "OlkfJqXYitYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "\n",
        "* **Group by Clusters:** We store detection IDs that belong to the same cluster in a list. This represents the detections that are identified as the same person across different cameras.\n",
        "* **Save to File:** The output is stored in the required format in `prediction.json`."
      ],
      "metadata": {
        "id": "GT1xrXn5i05r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics\n",
        "For this task, I could use clustering evaluation metrics such as:\n",
        "\n",
        "* **Adjusted Rand Index (ARI):** ARI measures the similarity between the true clusters and predicted clusters while adjusting for chance grouping. This would give me an objective way to evaluate how well the predicted clusters match the true labels, accounting for randomness in grouping.\n",
        "\n",
        "* **Fowlkes-Mallows Index (FMI):** FMI evaluates the quality of clustering based on both precision and recall. This would allow me to understand how accurately the algorithm identified clusters (precision) and how well it captured all instances of the same person (recall).\n",
        "\n",
        "Since I am not provided with explicit evaluation metrics or the ground truth labels for training, I could implement my own custom metric to evaluate how well the clustering groups similar detections based on their feature vectors.\n",
        "\n",
        "I could also experiment with the above metrics by comparing the predictions in `prediction.json` to the ground truth labels if they are available for evaluation. This would give me an indication of the clustering performance and allow me to further tune the model for better results."
      ],
      "metadata": {
        "id": "uZ4X-4UgjeQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Real-time Data\n",
        "If this task required real-time processing, I would need to modify my approach to handle streaming data efficiently.\n",
        "\n",
        "* **Batch Processing:** The current solution processes all the data at once, building a full similarity matrix and clustering the detections in one step. This approach is suitable when all the data is available upfront, allowing me to compute similarities for all pairs of detections and then apply a clustering algorithm.\n",
        "\n",
        "* **Real-time Processing:** In a real-time scenario, I would need to use an online clustering algorithm that can dynamically update clusters as new detections are received. As each new detection arrives, I would compare it to the existing clusters based on similarity. The detection would either be assigned to the closest cluster or, if no existing cluster is sufficiently similar, it would start a new cluster. Incremental clustering algorithms or nearest-neighbors-based methods, such as KD-trees or approximate nearest neighbors, could be applied to ensure efficient clustering with streaming data.\n",
        "\n",
        "* **Hybrid:** If I could wait for all the data (as in batch mode), I would focus on optimizing the clustering algorithm’s performance. This could include refining the distance threshold or incorporating additional metadata, such as bounding boxes, to improve the accuracy of predictions. For example, using additional spatial information alongside feature vectors could help disambiguate detections that are otherwise hard to distinguish.\n",
        "\n",
        "The current approach is optimized for batch processing, where the complete dataset is available before making predictions. For real-time applications, I would adopt a dynamic clustering approach that can efficiently update clusters as new data streams in, potentially using optimized nearest neighbor search techniques to handle real-time detection efficiently. In either case, further tuning of the clustering parameters—such as the distance threshold or switching to algorithms like DBSCAN—could further improve performance based on the data characteristics."
      ],
      "metadata": {
        "id": "bMIeNz4okl16"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxIrLZhdjAIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}